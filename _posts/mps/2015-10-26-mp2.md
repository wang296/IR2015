---
layout: article     
title: MP2&mdash; Statistical Language Model                   
category: mps    
author: Hongning Wang 
---

This assignment is designed to help you get familiar with statistical language models. You will get the basic ideas of maximum likelihood estimation, smoothing, generate text documents from language models, and language model evaluation. 

This assignment has in total **100** points. The deadline and submission guidance are explicitly described [here](#time).

# Data Set

We will use the same data set as used in our MP1. The data set is located at 

	"http://www.cs.virginia.edu/~hw5x/Course/IR2015/_site/docs/codes/data/yelp.zip" 

In the following discussion, this data set is referred as **train folder**. A separate (smaller) Yelp review set is provided for assignment. It can be downloaded from 

	"http://www.cs.virginia.edu/~hw5x/Course/IR2015/_site/docs/codes/data/yelp_test.zip" 

And this data set will be referred as **test folder** in the following discussions.

Following the same manner as in MP1, we will refer to each individual user review as a **document** (e.g., as in computing document frequency). You should reuse your JSON parser in this assignment.

The same pre-processing steps you have developed in MP1 will be used in this assignment, i.e., tokenization, stemming, and normalization. Note: **NO** stopword removal is needed in this assignment. 

# Statistical Language Models

### 1.1 Maximum likelihood estimation for statistical language models with proper smoothing (40pts)

Use all the review documents in the **train folder** to estimate a unigram language model $p(w)$ and two bigram language models (with different smoothing methods specified below). When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p\_u(w)$ to get two different bigram language models accordingly, i.e., $p^L(w\_i|w\_{i-1})$ and $p^A(w\_i|w\_{i-1})$. In linear interpolation smoothing, set the parameter $\lambda=0.9$; and in absolute discount smoothing, set the parameter $\delta=0.1$.

Specifically, when estimating $p^L(w\_i|w\_{i-1})$ and $p^A(w\_i|w\_{i-1})$, you should use the unigram language model $p(w_i)$ as the reference language model in smoothing. For example, in linear interpolation smoothing, the resulting smoothing formula looks like this,

$$p^L(w\_i|w\_{i-1})=(1-\lambda) p^{ML}(w\_i|w\_{i-1}) + \lambda p(w_i)$$

From the resulting two bigram language models, find the top 10 words that are most likely to follow the word "good", i.e., rank the words in a descending order by $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.

*HINT: to reduce space complexity, you do not need to actually maintain a $V\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$.* 

**What to submit**:

1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.
2. The top 10 words selected from the corresponding two bigram language models.
3. Your explanation of the observations about the top words under those two bigram language models.

### 2.2 Generate text documents from a language model (30pts)

Fixing the sentence length to be 15, generate 10 sentences by sampling words from $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$ respectively.

*HINT: you can use $p\_u(w)$ to generate the first word of a sentence and then sampling from the corresponding bigram language model when generating from $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$.* 

**What to submit**:

1. Paste your implementation of the sampling procedure from a language model.
2. The 10 sentences generated from $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$ accordingly, and the corresponding likelihood given by the used language model.

### 2.3 Language model evaluation (30pts)

Compute perplexity of the previously estimated language models, i.e., $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$, on all the review documents in the **test folder**.

The perplexity metric defined in our lecture slide is for one document (copied below). Follow that definition to compute perplexity for every review document in the test folder and compute the mean and standard deviation of the  resulting perplexities.

$PP(w\_1,w\_2,\dots,w\_n) = \sqrt[n]{\frac{1}{\prod\_{i=1}^n p(w\_i|w\_{i-1},\dots,w\_{i-N+1})}}$

where $d=w\_1,w\_2,\dots,w\_n$, i.e., a text sequence from testing document $d$ of length $n$; and the likelihood is computed under an N-gram language model.

*NOTE: to smooth the unseen words in the test folder, use additive smoothing to smooth the unigram language model $p\_u(w)$ by setting the parameter $\delta=0.1$. Then use the smoothed $\hat p\_u(w)$ to smooth $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$.*

**What to submit**:

1. Paste your implementation of the perplexity calculation of a language model.
2. Report the mean and standard deviation of the perplexities for $\hat p\_u(w)$, $\hat p^L\_b(w\_i|w\_{i-1})$ and $\hat p^A\_b(w\_i|w\_{i-1})$ on the test folder.     
3. Can you conclude which language model predicts the data in test folder better? And why?

# <a name="time"></a>Deadlines & How to submit

We have two deadlines: Part1 is due on **Feb. 12th 11:55pm**; and Part2 is due on **Feb. 19th 11:55pm**. Therefore, you have in total 14 days to finish this MP. The late policy for all our homework has been carefully discussed in the course syllabus.

Two separated collab assignment pages have been created for this MP. Please submit your written report strictly following the requirement specified above. The report **must be in PDF** format.  