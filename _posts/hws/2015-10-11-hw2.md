---
layout: article     
title: HW2&mdash; Basic Concepts about Probability and Linear Algebra        
category: hws        
author: Hongning Wang
description: HW2 for basic concepts about probability      
---

This homework assignment is designed to help you recall important concepts in probability and linear algebra. This assignment has in total **100** points. The deadline and submission guidance are explicitly described [here](#time).

## 1. Joint, marginal and conditional probabilities (35pts)

<center>
<table border="1" style="width:650px;">
	<tr>
		<td align="center"></td>
		<td align="center">$Y=1$</td>
		<td align="center">$Y=2$</td>
	</tr>
	<tr>
		<td align="center">$X=1$</td>
		<td align="center">1/3</td>
		<td align="center">1/12</td>
	</tr>
	<tr>
		<td align="center">$X=2$</td>
		<td align="center">1/6</td>
		<td align="center">0</td>
	</tr>
	<tr>
		<td align="center">$X=4$</td>
		<td align="center">1/12</td>
		<td align="center">1/3</td>
	</tr>
</table>
</center> 

1.1 Compute $P(X\le 2, Y>1)$. (5pts)             
1.2 Compute marginal probability mass function for $X$ and $Y$. (5pts)                 
1.3 Compute $P(Y=2|X=1)$. (5pts)       
1.4 Are $X$ and $Y$ independent? (5pts)             
1.5 Define $Z=X-2Y$, compute $P(X=2|Z=0)$. (5pts)             
1.6 Compute $E[X|Y=1]$. (5pts)                   
1.7 Compute $Var[X|Y=2]$. (5pts)                         


## 2. Proof of probabilistic ranking principle (15pts)

In our lecture discussion, we introduce the concept of probabilistic ranking principle from risk minimization perspective. However, we have not yet proved that ranking by the probability of being relevant will minimize the risk.         
Please finish the proof by assuming: 1) probability of a document being relevant is independent of the other documents; 2) users will sequentially browsing the results from top to bottom. Hint: what you need to prove is that whenever a user stops browsing, the scanned results are optimal in terms of those two types of risk, i.e., presenting an irrelevant result or missing a relevant result.      

## 3. Maximum likelihood estimation (20pts) 

The probability density function of Gaussian distribution is $f(x\|\mu,\sigma)=\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. Given a set of observations from this Gaussian distribution, $X=\{x\_1,x\_2,\dots,x\_n\}$, estimate the mean parameter $\mu$ and variance parameter $\sigma$ with details. 

## 4. Cosine v.s. Euclidean distance (15pts) 

$x$ and $y$ are two n-dimensional unit vectors, i.e., $\sum^n_{i=1}x^2\_i=1$. Figure out the relationship between the cosine similarity between $x$ and $y$ and Euclidean distance between $x$ and $y$. Hint: can you compute cosine similarity from Euclidean distance, and vice versus.

## 5. $\alpha_d$ in language model ranking modules (15pts) 

The generic ranking function based on smoothed language models can be written as,

$$\log P(q\|d)= \sum_{w\in d\cap q} \left(\frac{\log p(w\|d)}{\log \alpha\_{d} p(w\|C)} \right) + \|d\|\log\alpha\_{d}$$

where $p(w\|d)$ is the smoothed language model for document $d$, $p(w\|C)$ is a reference language model, and $\alpha\_{d}$ is a smoothing parameter defined in our lecture slides for "Refine the idea of smoothing."

Please derive the value of $\alpha\_{d}$ for Dirichlet prior smoothing. Hint: build the connection between $\alpha\_{d}$ and Dirichlet prior smoothing parameter $\mu$.

## <a name="time"></a> Deadline and How to submit

Please submit a **PDF** version of your solutions to our Collab site before the deadline. Your grade will be announced in Collab after the deadline.

The deadline for this assignment is 11:55pm, Monday, October 19th. 
   
